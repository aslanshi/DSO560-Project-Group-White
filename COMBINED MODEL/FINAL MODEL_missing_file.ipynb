{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\YuYao\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "import warnings\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('full_data_final version.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.iloc[:,np.r_[0:2,3:6,9:11]]\n",
    "df.drop_duplicates(inplace=True)\n",
    "df.set_index(['product_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Style - Nanchun (Aslan) Shi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select columns to be used for embedding model\n",
    "\n",
    "emb_df = df1.loc[:,['description','details']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import from self-created module; check Preprocessing.py for details\n",
    "\n",
    "from Preprocessing import embedding_preprocessing\n",
    "emb_pre = embedding_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "\n",
    "emb_vector_df = pd.DataFrame(emb_pre.preprocess(emb_df), index = emb_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'style_embedding_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-2575d5cdb2fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m## load embedding model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0memb_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'style_embedding_model.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'write'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_path_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'style_embedding_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "## load embedding model\n",
    "\n",
    "emb_model = load_model('style_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "\n",
    "emb_pred_vectors = emb_model.predict(emb_vector_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select columns to be used for tf-idf model\n",
    "\n",
    "tfidf_df = df1.loc[:,['brand','product_full_name','brand_category','brand_canonical_url']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import from self-created module; check Preprocessing.py for details\n",
    "\n",
    "from Preprocessing import tfidf_preprocessing\n",
    "tfidf_pre = tfidf_preprocessing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing\n",
    "\n",
    "tfidf_vector_df = tfidf_pre.preprocess(tfidf_df).set_index(tfidf_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load tf-idf model\n",
    "\n",
    "tfidf_model = load_model('style_tfidf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## predict\n",
    "\n",
    "tfidf_pred_vectors = tfidf_model.predict(tfidf_vector_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_classes(mat):\n",
    "    pred = list(map(lambda v: list(np.argsort(v))[-2:], mat))\n",
    "    return np.array(pred)\n",
    "\n",
    "label_dict = load_obj('style_label_dict_rev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_vectors = 0.4*emb_pred_vectors + 0.6*tfidf_pred_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_pred_classes = get_pred_classes(final_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['style_prediction'] = list(map(lambda x: [label_dict[x[0]], label_dict[x[1]]], final_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Fit - Xinyi (Alex) Guo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('full_data_final version.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(text, punctuations=string.punctuation+\"``\"+\"’\"+\"”\"):\n",
    "    words=nltk.word_tokenize(text)\n",
    "    newWords = [word for word in words if word.lower() not in punctuations]\n",
    "    cleanedText = \" \".join(newWords)\n",
    "    return cleanedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = set(stopwords.words(\"English\"))\n",
    "def removeStopwords(text, stopwords=nltk_stopwords):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    newWords = [word for word in words if word.lower() not in stopwords]\n",
    "    cleanedText = \" \".join(newWords)\n",
    "    return cleanedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatizedWords = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "    lemmatizedText = \" \".join(lemmatizedWords)\n",
    "    return lemmatizedText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df, columns = [\"brand\", \"product_full_name\", \"description\", \"details\"]):\n",
    "    df['details'] = df['details'].str.replace(\"\\n\", \"\")\n",
    "    #replace null values with UNKNOWN_TOKEN\n",
    "    df['brand'] = df['brand'].fillna('UNKNOWN_TOKEN')\n",
    "    df['description'] = df['description'].fillna('UNKNOWN_TOKEN')\n",
    "    df['details'] = df['details'].fillna('UNKNOWN_TOKEN')\n",
    "    df['product_full_name'] = df['product_full_name'].fillna('UNKNOWN_TOKEN')\n",
    "    #remove punctuation and stopwords then lemmatize\n",
    "    for col in columns: \n",
    "        df[col] = df[col].apply(removePunctuation)\n",
    "        df[col] = df[col].apply(removeStopwords)\n",
    "        df[col] = df[col].apply(lemmatize)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Keras Modeling Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_max_token_length_per_doc(docs):\n",
    "    return max(list(map(lambda x: len(x.split()), docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test_df, target, max_length):\n",
    "    #load data\n",
    "    X_test_df[\"input_doc\"] = X_test_df.brand + \" \" + X_test_df.product_full_name + \" \" \\\n",
    "                                + X_test_df.description + \" \" + X_test_df.details \n",
    "    X_test = X_test_df.loc[:, \"input_doc\"].values\n",
    "    test_docs = list(X_test)\n",
    "\n",
    "    #load model\n",
    "    model = load_model(\"{}_model.h5\".format(target))\n",
    "    with open('{}_tokenizer.pickle'.format(target), 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "        \n",
    "    #predict\n",
    "    encoded_test_docs = integer_encode_documents(test_docs, tokenizer)\n",
    "    padded_test_docs = pad_sequences(encoded_test_docs, maxlen=max_length, padding='post')\n",
    "    prediction_proba = model.predict(padded_test_docs, verbose = 0)\n",
    "    \n",
    "    return prediction_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(df):\n",
    "    '''\n",
    "    This function will predict the fit of the clothing. It takes a dataframe as an input. The CSV file needs to have \n",
    "    \"brand\", \"product_full_name\", \"description\", and \"details\" columns. The function will output a dataframe with an \n",
    "    additional fit column. \n",
    "    '''\n",
    "    #load data\n",
    "#     inputFile = input(\"What's the name of the csv file? (ex. full_data.csv)\")\n",
    "    fullData = df\n",
    "    #Preprocess data\n",
    "    print(\"Start preprocessing data...\")\n",
    "    testData = fullData.copy()\n",
    "    testData = testData.loc[:, [\"brand\", \"product_full_name\", \"description\", \"details\"]]\n",
    "    testData = preprocessing(testData)\n",
    "    print(\"Start predicting fit...\")\n",
    "    #Predict fit\n",
    "    maxLengthDict = {'straightregular': 185,\n",
    "                 'semifitted': 185,\n",
    "                 'relaxed': 202,\n",
    "                 'oversized': 202,\n",
    "                 'fittedtailored': 202}\n",
    "    prob_df = pd.DataFrame()\n",
    "    fitType = ['straightregular', 'semifitted', 'relaxed', 'oversized', 'fittedtailored']\n",
    "    for fit in fitType:\n",
    "        prediction_proba = predict(testData, target = fit, max_length = maxLengthDict[fit])\n",
    "        prob_df[fit] = prediction_proba.flatten()\n",
    "        print(fit, \"fit prediction done\")\n",
    "    prob_df['predict_fit'] = prob_df.idxmax(axis=1)\n",
    "    fullData['fit'] = prob_df['predict_fit']\n",
    "#     fullData.to_csv(\"full_data with fit prediction.csv\")\n",
    "    return fullData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preprocessing data...\n",
      "Start predicting fit...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = 'straightregular_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-aa5f85a300fc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-18-153c2c5399dd>\u001b[0m in \u001b[0;36mmain\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mfitType\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'straightregular'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'semifitted'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'relaxed'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'oversized'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fittedtailored'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mfit\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfitType\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mprediction_proba\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtestData\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaxLengthDict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mprob_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction_proba\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"fit prediction done\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-6a56084a1ae2>\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(X_test_df, target, max_length)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;31m#load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}_model.h5\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'{}_tokenizer.pickle'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    490\u001b[0m                 \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtmp_filepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    491\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 492\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mload_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    493\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    494\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mload_wrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_supported_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 583\u001b[1;33m         \u001b[1;32mwith\u001b[0m \u001b[0mH5Dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mh5dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    584\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_deserialize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh5dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_objects\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    585\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'write'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\utils\\io_utils.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, path, mode)\u001b[0m\n\u001b[0;32m    189\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_is_path_instance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5py\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = 'straightregular_model.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "df2 = main(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III. Occasion - Bingru Xue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in df3.columns:\n",
    "    df3[i] = df3[i].str.lower()\n",
    "df3= df3.replace(np.nan, 'UNKNOWN_TOKEN', regex=True)\n",
    "df3['details'] = df3['details'].str.replace(\"\\n\", \"\")\n",
    "df3['text'] = df3['description']+' '+df3['details']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_md')\n",
    "\n",
    "def preprocess_text(sen):\n",
    "    # Remove punctuations and numbers\n",
    "    sentence = re.sub('[^a-zA-Z]', ' ', sen)\n",
    "\n",
    "    # Single character removal\n",
    "    sentence = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', sentence)\n",
    "\n",
    "    # Removing multiple spaces\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence)\n",
    "    \n",
    "    #remove stopwords and do lemmatization\n",
    "    doc = nlp(sentence)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df3['text'] = df3['text'].apply(preprocess_text)\n",
    "df3['product_full_name'] = df3['product_full_name'].apply(preprocess_text)\n",
    "df3['brand_category'] = df3['brand_category'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_url(url):\n",
    "    url = re.sub('https://www.', '', url)\n",
    "    url = re.sub('.com', '', url)\n",
    "    url = re.sub('/', ' ', url)\n",
    "    url = re.sub('-', ' ', url)\n",
    "    url = re.sub(r'[0-9]+', ' ', url)\n",
    "    url = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', url)\n",
    "    url = re.sub(r'\\s+', ' ', url)\n",
    "\n",
    "    doc = nlp(url)\n",
    "    tokens = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df3['brand_canonical_url'] = df3['brand_canonical_url'].apply(preprocess_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['brand_info'] = df3['brand']+' '+df3['product_full_name']+' '+\\\n",
    "                    df3['brand_category']+' '+df3['brand_canonical_url']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Embedding Model: Description & Detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df3['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "product_id\n",
       "01DSE9TC2DQXDG6GWKW9NMJ416    modern pump rounded silhouette ankle strap ext...\n",
       "01DSE9SKM19XNA6SJP36JZC065    dress jean sneaker dress tailor trouser heel t...\n",
       "01DSJX8GD4DSAP76SPR85HRCMN    padded leather cover classic round sunglass uv...\n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2    iconic mid design get add dose support padded ...\n",
       "01DSK15ZD4D5A0QXA8NSD25YXE    UNKNOWN_TOKEN shade offer UNKNOWN_TOKEN view i...\n",
       "                                                    ...                        \n",
       "01DSNVXY8EJ9FQAJ3MPDMPASHD    unknown token cozy double breasted jacket craf...\n",
       "01DSGYHA3RMCHENBJVQPBGXM97    UNKNOWN_TOKEN hour long wear water resistant U...\n",
       "01DSJT8H12CAFQQH07SQSQWJ8C    ruffled trim sweatshirt lend romance stripe le...\n",
       "01DSH2PF9J7QZ44D842B3GMCFN    pretty plaid dress velvet collar velvet bow po...\n",
       "01DSH54D3PWHKFZK5A8A2JE3RQ    unknown token corduroy dress bow cotton cordur...\n",
       "Name: text, Length: 48090, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokenizer = load_obj(\"occasion_tokeniver\")\n",
    "new_doc = []\n",
    "def replace_oov(sentence):\n",
    "    now_sen =[]\n",
    "    for word in word_tokenize(sentence):\n",
    "        if word in tokenizer.word_index.keys():\n",
    "            now_sen.append(word)\n",
    "        else:\n",
    "            now_sen.append(\"UNKNOWN_TOKEN\")\n",
    "    return \" \".join(now_sen)\n",
    "docs.apply(replace_oov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "token = tokenizer.texts_to_sequences(docs)\n",
    "pad = pad_sequences(token, padding='post', maxlen=165, truncating='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = load_model('occasion_embedding_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "occasion_type = [\"cold weather\",\"day to night\",\"night out\",\"vacation\",\"weekend\",\"work\",\"workout\"]\n",
    "pred = embedding_model.predict(pad)\n",
    "embedding_df = pd.DataFrame(data= pred, columns = occasion_type,index=df3.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Vectorization Model: Brand, Name, URL, Brand Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = df3['brand_info']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = load_obj(\"occasion_vectorizer\")\n",
    "vector = vectorizer.transform(info)\n",
    "tf_idf_df = pd.DataFrame(vector.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "occasion_type = [\"cold weather\",\"day to night\",\"night out\",\"vacation\",\"weekend\",\"work\",\"workout\"]\n",
    "vector_df = pd.DataFrame(columns = occasion_type, index = df3.index)\n",
    "\n",
    "for label in occasion_type:\n",
    "    filename = \"{}\".format(label)+\"_vector_model\"\n",
    "    vector_model = load_obj(filename)\n",
    "    prob = vector_model.predict_proba(vector)[:,1]\n",
    "    vector_df[label] = prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Combine embedding model and vector model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "occasion_result_df = 0.4*embedding_df + 0.6*vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cold weather</th>\n",
       "      <th>day to night</th>\n",
       "      <th>night out</th>\n",
       "      <th>vacation</th>\n",
       "      <th>weekend</th>\n",
       "      <th>work</th>\n",
       "      <th>workout</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01DSE9TC2DQXDG6GWKW9NMJ416</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSE9SKM19XNA6SJP36JZC065</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSJX8GD4DSAP76SPR85HRCMN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSJVKJNS6F4KQ1QM6YYK9AW2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSK15ZD4D5A0QXA8NSD25YXE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSNVXY8EJ9FQAJ3MPDMPASHD</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSGYHA3RMCHENBJVQPBGXM97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSJT8H12CAFQQH07SQSQWJ8C</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSH2PF9J7QZ44D842B3GMCFN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSH54D3PWHKFZK5A8A2JE3RQ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48090 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            cold weather  day to night  night out  vacation  \\\n",
       "product_id                                                                    \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416           0.0           1.0        0.0       0.0   \n",
       "01DSE9SKM19XNA6SJP36JZC065           0.0           1.0        0.0       0.0   \n",
       "01DSJX8GD4DSAP76SPR85HRCMN           0.0           0.0        0.0       1.0   \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2           0.0           1.0        0.0       0.0   \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE           0.0           0.0        0.0       0.0   \n",
       "...                                  ...           ...        ...       ...   \n",
       "01DSNVXY8EJ9FQAJ3MPDMPASHD           1.0           1.0        0.0       0.0   \n",
       "01DSGYHA3RMCHENBJVQPBGXM97           0.0           0.0        0.0       1.0   \n",
       "01DSJT8H12CAFQQH07SQSQWJ8C           0.0           1.0        0.0       0.0   \n",
       "01DSH2PF9J7QZ44D842B3GMCFN           0.0           0.0        0.0       0.0   \n",
       "01DSH54D3PWHKFZK5A8A2JE3RQ           0.0           0.0        0.0       0.0   \n",
       "\n",
       "                            weekend  work  workout  \n",
       "product_id                                          \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416      0.0   0.0      0.0  \n",
       "01DSE9SKM19XNA6SJP36JZC065      1.0   1.0      0.0  \n",
       "01DSJX8GD4DSAP76SPR85HRCMN      1.0   0.0      0.0  \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2      1.0   0.0      0.0  \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE      1.0   0.0      0.0  \n",
       "...                             ...   ...      ...  \n",
       "01DSNVXY8EJ9FQAJ3MPDMPASHD      1.0   0.0      0.0  \n",
       "01DSGYHA3RMCHENBJVQPBGXM97      1.0   0.0      0.0  \n",
       "01DSJT8H12CAFQQH07SQSQWJ8C      1.0   0.0      0.0  \n",
       "01DSH2PF9J7QZ44D842B3GMCFN      1.0   0.0      0.0  \n",
       "01DSH54D3PWHKFZK5A8A2JE3RQ      1.0   0.0      0.0  \n",
       "\n",
       "[48090 rows x 7 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decision(probs):\n",
    "    if sum(probs>0.5)>0:\n",
    "        probs[probs > 0.5] = 1\n",
    "        probs[probs <= 0.5] = 0\n",
    "    else:\n",
    "        probs[probs == np.max(probs)] = 1\n",
    "        probs[probs != np.max(probs)] = 0\n",
    "    return probs\n",
    "\n",
    "occasion_result_df.apply(decision, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "occasion = []\n",
    "for i in occasion_result_df.index:\n",
    "    label = []\n",
    "    for j in occasion_result_df.columns:\n",
    "        if occasion_result_df.loc[i,j].any():\n",
    "            label.append(j)\n",
    "    occasion.append(label)\n",
    "occasion_result_df['Occasion'] = occasion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cold weather</th>\n",
       "      <th>day to night</th>\n",
       "      <th>night out</th>\n",
       "      <th>vacation</th>\n",
       "      <th>weekend</th>\n",
       "      <th>work</th>\n",
       "      <th>workout</th>\n",
       "      <th>Occasion</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01DSE9TC2DQXDG6GWKW9NMJ416</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[day to night]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSE9SKM19XNA6SJP36JZC065</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[day to night, weekend, work]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSJX8GD4DSAP76SPR85HRCMN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[vacation, weekend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSJVKJNS6F4KQ1QM6YYK9AW2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[day to night, weekend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSK15ZD4D5A0QXA8NSD25YXE</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[weekend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSNVXY8EJ9FQAJ3MPDMPASHD</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[cold weather, day to night, weekend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSGYHA3RMCHENBJVQPBGXM97</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[vacation, weekend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSJT8H12CAFQQH07SQSQWJ8C</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[day to night, weekend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSH2PF9J7QZ44D842B3GMCFN</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[weekend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSH54D3PWHKFZK5A8A2JE3RQ</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[weekend]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>48090 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            cold weather  day to night  night out  vacation  \\\n",
       "product_id                                                                    \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416           0.0           1.0        0.0       0.0   \n",
       "01DSE9SKM19XNA6SJP36JZC065           0.0           1.0        0.0       0.0   \n",
       "01DSJX8GD4DSAP76SPR85HRCMN           0.0           0.0        0.0       1.0   \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2           0.0           1.0        0.0       0.0   \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE           0.0           0.0        0.0       0.0   \n",
       "...                                  ...           ...        ...       ...   \n",
       "01DSNVXY8EJ9FQAJ3MPDMPASHD           1.0           1.0        0.0       0.0   \n",
       "01DSGYHA3RMCHENBJVQPBGXM97           0.0           0.0        0.0       1.0   \n",
       "01DSJT8H12CAFQQH07SQSQWJ8C           0.0           1.0        0.0       0.0   \n",
       "01DSH2PF9J7QZ44D842B3GMCFN           0.0           0.0        0.0       0.0   \n",
       "01DSH54D3PWHKFZK5A8A2JE3RQ           0.0           0.0        0.0       0.0   \n",
       "\n",
       "                            weekend  work  workout  \\\n",
       "product_id                                           \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416      0.0   0.0      0.0   \n",
       "01DSE9SKM19XNA6SJP36JZC065      1.0   1.0      0.0   \n",
       "01DSJX8GD4DSAP76SPR85HRCMN      1.0   0.0      0.0   \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2      1.0   0.0      0.0   \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE      1.0   0.0      0.0   \n",
       "...                             ...   ...      ...   \n",
       "01DSNVXY8EJ9FQAJ3MPDMPASHD      1.0   0.0      0.0   \n",
       "01DSGYHA3RMCHENBJVQPBGXM97      1.0   0.0      0.0   \n",
       "01DSJT8H12CAFQQH07SQSQWJ8C      1.0   0.0      0.0   \n",
       "01DSH2PF9J7QZ44D842B3GMCFN      1.0   0.0      0.0   \n",
       "01DSH54D3PWHKFZK5A8A2JE3RQ      1.0   0.0      0.0   \n",
       "\n",
       "                                                         Occasion  \n",
       "product_id                                                         \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416                         [day to night]  \n",
       "01DSE9SKM19XNA6SJP36JZC065          [day to night, weekend, work]  \n",
       "01DSJX8GD4DSAP76SPR85HRCMN                    [vacation, weekend]  \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2                [day to night, weekend]  \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE                              [weekend]  \n",
       "...                                                           ...  \n",
       "01DSNVXY8EJ9FQAJ3MPDMPASHD  [cold weather, day to night, weekend]  \n",
       "01DSGYHA3RMCHENBJVQPBGXM97                    [vacation, weekend]  \n",
       "01DSJT8H12CAFQQH07SQSQWJ8C                [day to night, weekend]  \n",
       "01DSH2PF9J7QZ44D842B3GMCFN                              [weekend]  \n",
       "01DSH54D3PWHKFZK5A8A2JE3RQ                              [weekend]  \n",
       "\n",
       "[48090 rows x 8 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "occasion_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "occasion_final = occasion_result_df.loc[:,\"Occasion\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IV. Patterns/Prints - Jiayue (Daniel) Chen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import load_model\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data = pd.read_csv('full_data_final version.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#replace null values with UNKNOWN_TOKEN\n",
    "full_data['description'] = full_data['description'].fillna('UNKNOWN_TOKEN')\n",
    "full_data['details'] = full_data['details'].fillna('UNKNOWN_TOKEN')\n",
    "full_data['brand_category'] = full_data['brand_category'].fillna('UNKNOWN_TOKEN')\n",
    "full_data['details'] = full_data['details'].str.replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string \n",
    "# define a function to remove punctuation\n",
    "def removePunctuation(text, punctuations=string.punctuation+\"``\"+\"’\"+\"”\"):\n",
    "    words=nltk.word_tokenize(text)\n",
    "    newWords = [word for word in words if word.lower() not in punctuations]\n",
    "    cleanedText = \" \".join(newWords)\n",
    "    return cleanedText\n",
    "\n",
    "nltk_stopwords = set(stopwords.words(\"English\"))\n",
    "\n",
    "# define a function to remove stopwords\n",
    "def removeStopwords(text, stopwords=nltk_stopwords):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    newWords = [word for word in words if word.lower() not in stopwords]\n",
    "    cleanedText = \" \".join(newWords)\n",
    "    return cleanedText\n",
    "\n",
    "# define a function to lemmatize all texts\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemmatizedWords = [lemmatizer.lemmatize(word.lower()) for word in words]\n",
    "    lemmatizedText = \" \".join(lemmatizedWords)\n",
    "    return lemmatizedText\n",
    "\n",
    "columns = [\"brand\", \"product_full_name\", \"description\", \"details\"]\n",
    "for col in columns: \n",
    "    full_data[col] = full_data[col].apply(removePunctuation)\n",
    "    full_data[col] = full_data[col].apply(removeStopwords)\n",
    "    full_data[col] = full_data[col].apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_data['abstract'] = 0\n",
    "full_data['animal'] = 0\n",
    "full_data['camouflage'] = 0\n",
    "full_data['colorblock'] = 0\n",
    "full_data['dots'] = 0\n",
    "full_data['floral'] = 0\n",
    "full_data['geometric'] = 0\n",
    "full_data['graphic'] = 0\n",
    "full_data['houndstooth'] = 0\n",
    "full_data['logo'] = 0\n",
    "full_data['monogram'] = 0\n",
    "full_data['multiprint'] = 0\n",
    "full_data['paisley'] = 0\n",
    "full_data['pinstripe'] = 0\n",
    "full_data['plaid'] = 0\n",
    "full_data['stripe'] = 0\n",
    "full_data['stripehorizontal'] = 0\n",
    "full_data['stripevertical'] = 0\n",
    "full_data['tiedye'] = 0\n",
    "full_data['tropical'] = 0\n",
    "full_data[\"input_doc\"] = full_data.brand+\" \"+full_data.product_full_name+\" \"+full_data.description+\" \"+full_data.details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Modeling & Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integer_encode_documents(docs, tokenizer):\n",
    "    return tokenizer.texts_to_sequences(docs)\n",
    "\n",
    "def get_max_token_length_per_doc(docs):\n",
    "    return max(list(map(lambda x: len(x.split()), docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the complete pattern's file, we generate a list of max_length\n",
    "max_length_list = [150, 150, 124, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern_values = list(full_data.columns.values[13:33])\n",
    "for i in range(13, 33):\n",
    "    X_test = full_data.iloc[:, -1].values\n",
    "    test_docs = list(X_test)\n",
    "    model = load_model(\"{}_model.h5\".format(pattern_values[i-13]))\n",
    "    with open('{}_tokenizer.pickle'.format(pattern_values[i-13]), 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "        \n",
    "   #predict\n",
    "    encoded_test_docs = integer_encode_documents(test_docs, tokenizer)\n",
    "    padded_test_docs = pad_sequences(encoded_test_docs, maxlen = max_length_list[i-13], padding = 'post')\n",
    "    prediction = model.predict(padded_test_docs, verbose = 0)\n",
    "    full_data.iloc[:, i] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = full_data.iloc[:, 13:33]\n",
    "df4['patterns/prints'] = df4.idxmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>brand</th>\n",
       "      <th>mpn</th>\n",
       "      <th>product_full_name</th>\n",
       "      <th>description</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>created_at</th>\n",
       "      <th>updated_at</th>\n",
       "      <th>deleted_at</th>\n",
       "      <th>brand_canonical_url</th>\n",
       "      <th>details</th>\n",
       "      <th>labels</th>\n",
       "      <th>bc_product_id</th>\n",
       "      <th>patterns/prints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>01DSE9TC2DQXDG6GWKW9NMJ416</td>\n",
       "      <td>Banana Republic</td>\n",
       "      <td>514683</td>\n",
       "      <td>Ankle-Strap Pump</td>\n",
       "      <td>A modern pump, in a rounded silhouette with an...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2019-11-11 22:37:15.719107+00</td>\n",
       "      <td>2019-12-19 20:40:30.786144+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://bananarepublic.gap.com/browse/product....</td>\n",
       "      <td>A modern pump, in a rounded silhouette with an...</td>\n",
       "      <td>{\"Needs Review\"}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>animal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>01DSE9SKM19XNA6SJP36JZC065</td>\n",
       "      <td>Banana Republic</td>\n",
       "      <td>526676</td>\n",
       "      <td>Petite Tie-Neck Top</td>\n",
       "      <td>Dress it down with jeans and sneakers or dress...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>2019-11-11 22:36:50.682513+00</td>\n",
       "      <td>2019-12-19 20:40:30.786144+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://bananarepublic.gap.com/browse/product....</td>\n",
       "      <td>Dress it down with jeans and sneakers or dress...</td>\n",
       "      <td>{\"Needs Review\"}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>floral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>01DSJX8GD4DSAP76SPR85HRCMN</td>\n",
       "      <td>Loewe</td>\n",
       "      <td>4.001E+11</td>\n",
       "      <td>52MM Padded Leather Round Sunglasses</td>\n",
       "      <td>Padded leather covers classic round sunglasses.</td>\n",
       "      <td>JewelryAccessories/SunglassesReaders/RoundOval...</td>\n",
       "      <td>2019-11-13 17:33:59.581661+00</td>\n",
       "      <td>2019-12-19 20:40:30.786144+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.saksfifthavenue.com/loewe-52mm-pad...</td>\n",
       "      <td>100% UV protection Case and cleaning cloth inc...</td>\n",
       "      <td>{\"Needs Review\"}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>01DSJVKJNS6F4KQ1QM6YYK9AW2</td>\n",
       "      <td>Converse</td>\n",
       "      <td>4.00012E+11</td>\n",
       "      <td>Baby's &amp; Little Kid's All-Star Two-Tone Mid-To...</td>\n",
       "      <td>The iconic mid-top design gets an added dose o...</td>\n",
       "      <td>JustKids/Shoes/Baby024Months/BabyGirl,JustKids...</td>\n",
       "      <td>2019-11-13 17:05:05.203733+00</td>\n",
       "      <td>2019-12-19 20:40:30.786144+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.saksfifthavenue.com/converse-babys...</td>\n",
       "      <td>Canvas upper Round toe Lace-up vamp SmartFOAM ...</td>\n",
       "      <td>{\"Needs Review\"}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>logo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>01DSK15ZD4D5A0QXA8NSD25YXE</td>\n",
       "      <td>Alexander McQueen</td>\n",
       "      <td>4.00011E+11</td>\n",
       "      <td>64MM Rimless Sunglasses</td>\n",
       "      <td>Hexagonal shades offer a rimless view with int...</td>\n",
       "      <td>JewelryAccessories/SunglassesReaders/RoundOval</td>\n",
       "      <td>2019-11-13 18:42:30.941321+00</td>\n",
       "      <td>2019-12-19 20:40:30.786144+00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://www.saksfifthavenue.com/alexander-mcqu...</td>\n",
       "      <td>100% UV protection Gradient lenses Adjustable ...</td>\n",
       "      <td>{\"Needs Review\"}</td>\n",
       "      <td>NaN</td>\n",
       "      <td>animal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   product_id              brand          mpn  \\\n",
       "0  01DSE9TC2DQXDG6GWKW9NMJ416    Banana Republic       514683   \n",
       "1  01DSE9SKM19XNA6SJP36JZC065    Banana Republic       526676   \n",
       "2  01DSJX8GD4DSAP76SPR85HRCMN              Loewe    4.001E+11   \n",
       "3  01DSJVKJNS6F4KQ1QM6YYK9AW2           Converse  4.00012E+11   \n",
       "4  01DSK15ZD4D5A0QXA8NSD25YXE  Alexander McQueen  4.00011E+11   \n",
       "\n",
       "                                   product_full_name  \\\n",
       "0                                   Ankle-Strap Pump   \n",
       "1                                Petite Tie-Neck Top   \n",
       "2               52MM Padded Leather Round Sunglasses   \n",
       "3  Baby's & Little Kid's All-Star Two-Tone Mid-To...   \n",
       "4                            64MM Rimless Sunglasses   \n",
       "\n",
       "                                         description  \\\n",
       "0  A modern pump, in a rounded silhouette with an...   \n",
       "1  Dress it down with jeans and sneakers or dress...   \n",
       "2    Padded leather covers classic round sunglasses.   \n",
       "3  The iconic mid-top design gets an added dose o...   \n",
       "4  Hexagonal shades offer a rimless view with int...   \n",
       "\n",
       "                                      brand_category  \\\n",
       "0                                            Unknown   \n",
       "1                                            Unknown   \n",
       "2  JewelryAccessories/SunglassesReaders/RoundOval...   \n",
       "3  JustKids/Shoes/Baby024Months/BabyGirl,JustKids...   \n",
       "4     JewelryAccessories/SunglassesReaders/RoundOval   \n",
       "\n",
       "                      created_at                     updated_at deleted_at  \\\n",
       "0  2019-11-11 22:37:15.719107+00  2019-12-19 20:40:30.786144+00        NaN   \n",
       "1  2019-11-11 22:36:50.682513+00  2019-12-19 20:40:30.786144+00        NaN   \n",
       "2  2019-11-13 17:33:59.581661+00  2019-12-19 20:40:30.786144+00        NaN   \n",
       "3  2019-11-13 17:05:05.203733+00  2019-12-19 20:40:30.786144+00        NaN   \n",
       "4  2019-11-13 18:42:30.941321+00  2019-12-19 20:40:30.786144+00        NaN   \n",
       "\n",
       "                                 brand_canonical_url  \\\n",
       "0  https://bananarepublic.gap.com/browse/product....   \n",
       "1  https://bananarepublic.gap.com/browse/product....   \n",
       "2  https://www.saksfifthavenue.com/loewe-52mm-pad...   \n",
       "3  https://www.saksfifthavenue.com/converse-babys...   \n",
       "4  https://www.saksfifthavenue.com/alexander-mcqu...   \n",
       "\n",
       "                                             details            labels  \\\n",
       "0  A modern pump, in a rounded silhouette with an...  {\"Needs Review\"}   \n",
       "1  Dress it down with jeans and sneakers or dress...  {\"Needs Review\"}   \n",
       "2  100% UV protection Case and cleaning cloth inc...  {\"Needs Review\"}   \n",
       "3  Canvas upper Round toe Lace-up vamp SmartFOAM ...  {\"Needs Review\"}   \n",
       "4  100% UV protection Gradient lenses Adjustable ...  {\"Needs Review\"}   \n",
       "\n",
       "   bc_product_id patterns/prints  \n",
       "0            NaN          animal  \n",
       "1            NaN          floral  \n",
       "2            NaN            logo  \n",
       "3            NaN            logo  \n",
       "4            NaN          animal  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_data = pd.read_csv('full_data_final version.csv')\n",
    "full_data['patterns/prints'] = df4['patterns/prints']\n",
    "full_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V. Category - Yuyao Shen \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = string.punctuation\n",
    "nlp = spacy.load('en')\n",
    "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "from spacy.lang.en import English\n",
    "parser = English()\n",
    "\n",
    "def clean_text(text):\n",
    "    '''\n",
    "    use regular expression to clean text \n",
    "    replace numbers and units to variables\n",
    "    '''\n",
    "    p = re.compile(r'<.*?>')\n",
    "    text = p.sub('', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\xa0', '',text)\n",
    "    text = re.sub(r'\\d{1,3}(\\.|\\’)?\\d{1,3}?(\\\"|\\”)',\"length_val\", text)\n",
    "    text = re.sub(r'\\d{1,3}\\s*?%',\"percentage_val\", text)\n",
    "    text = text.strip(string.punctuation).replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = re.sub(r'\\d{1,3}\\s*?mm',\"mm_val\", text)\n",
    "    text = re.sub(r'\\d{1,3}\\s*?cm',\"cm_val\", text)\n",
    "    text = re.sub(r'\\d{1,3}\\s*?(inches|inch)',\"inches_val\", text)\n",
    "    text = re.sub(r'\\d{1,3}\\s*?(lbs|kg)',\"weight_val\", text)\n",
    "    text = re.sub(r'size\\s*?\\d{1,3}\\s*?',\"size_val\", text)\n",
    "    text = re.sub(r'\\b\\d+\\b',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text) \n",
    "    mytokens = parser(text)\n",
    "    mytokens = [ word.lemma_.lower().strip() for word in mytokens ]\n",
    "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
    "    return \" \".join(mytokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_cate_test(inputFile):\n",
    "    '''\n",
    "    Input testing dataset that is going to be labeled\n",
    "    Keep relevant columns \n",
    "    Basic cleaning\n",
    "    Output cleaned testing dataset for category in a dataframe\n",
    "    '''\n",
    "    #full_test_data = pd.read_csv(inputFile)\n",
    "    full_test_data = df5  \n",
    "    ### keep relevant columns only\n",
    "    test_data = full_test_data[['product_full_name', 'details','description', 'brand_category']]\n",
    "    ### fill null values with 'Unknown_token'\n",
    "    test_data.fillna('Unknown_token', inplace = True)\n",
    "    X_test = test_data['product_full_name'] + ' '+ test_data['details'] + ' '+test_data['description']+ ' '+test_data['brand_category']\n",
    "    return test_data, X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_classes(mat):\n",
    "    pred = list(map(lambda v: list(np.argsort(v))[-1:], mat))\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_data, X_test = clean_cate_test('full_data_final version.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading vectorizer\n",
    "Pkl_Filename = \"category_token.pkl\"  \n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    tk = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "### vectorize incoming data\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "vector_text_test = tk.texts_to_sequences(X_test)\n",
    "padded_token_lists_test = pad_sequences(vector_text_test, maxlen=175, padding='post')\n",
    "X_test = pd.DataFrame(padded_token_lists_test, index = full_test_data.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3  Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "### loading model\n",
    "Pkl_Filename = \"category_model.pkl\"  \n",
    "with open(Pkl_Filename, 'rb') as file:  \n",
    "    model = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use trained model to predict incoming data\n",
    "pred_vectors_test = model.predict(X_test)\n",
    "test_pred_classes = get_pred_classes(pred_vectors_test)\n",
    "categories = ['accessory', 'bottom', 'onepiece', 'shoe', 'top']\n",
    "cate_pred = [categories[i[0]] for i in test_pred_classes]\n",
    "predicted_test = pd.Series(cate_pred).str.capitalize() \n",
    "df5['category']  = list(predicted_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brand</th>\n",
       "      <th>product_full_name</th>\n",
       "      <th>description</th>\n",
       "      <th>brand_category</th>\n",
       "      <th>brand_canonical_url</th>\n",
       "      <th>details</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>product_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>01DSE9TC2DQXDG6GWKW9NMJ416</th>\n",
       "      <td>Banana Republic</td>\n",
       "      <td>Ankle-Strap Pump</td>\n",
       "      <td>A modern pump, in a rounded silhouette with an...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>https://bananarepublic.gap.com/browse/product....</td>\n",
       "      <td>A modern pump, in a rounded silhouette with an...</td>\n",
       "      <td>Shoe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSE9SKM19XNA6SJP36JZC065</th>\n",
       "      <td>Banana Republic</td>\n",
       "      <td>Petite Tie-Neck Top</td>\n",
       "      <td>Dress it down with jeans and sneakers or dress...</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>https://bananarepublic.gap.com/browse/product....</td>\n",
       "      <td>Dress it down with jeans and sneakers or dress...</td>\n",
       "      <td>Onepiece</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSJX8GD4DSAP76SPR85HRCMN</th>\n",
       "      <td>Loewe</td>\n",
       "      <td>52MM Padded Leather Round Sunglasses</td>\n",
       "      <td>Padded leather covers classic round sunglasses.</td>\n",
       "      <td>JewelryAccessories/SunglassesReaders/RoundOval...</td>\n",
       "      <td>https://www.saksfifthavenue.com/loewe-52mm-pad...</td>\n",
       "      <td>100% UV protection Case and cleaning cloth inc...</td>\n",
       "      <td>Accessory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSJVKJNS6F4KQ1QM6YYK9AW2</th>\n",
       "      <td>Converse</td>\n",
       "      <td>Baby's &amp; Little Kid's All-Star Two-Tone Mid-To...</td>\n",
       "      <td>The iconic mid-top design gets an added dose o...</td>\n",
       "      <td>JustKids/Shoes/Baby024Months/BabyGirl,JustKids...</td>\n",
       "      <td>https://www.saksfifthavenue.com/converse-babys...</td>\n",
       "      <td>Canvas upper Round toe Lace-up vamp SmartFOAM ...</td>\n",
       "      <td>Shoe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>01DSK15ZD4D5A0QXA8NSD25YXE</th>\n",
       "      <td>Alexander McQueen</td>\n",
       "      <td>64MM Rimless Sunglasses</td>\n",
       "      <td>Hexagonal shades offer a rimless view with int...</td>\n",
       "      <td>JewelryAccessories/SunglassesReaders/RoundOval</td>\n",
       "      <td>https://www.saksfifthavenue.com/alexander-mcqu...</td>\n",
       "      <td>100% UV protection Gradient lenses Adjustable ...</td>\n",
       "      <td>Accessory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        brand  \\\n",
       "product_id                                      \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416    Banana Republic   \n",
       "01DSE9SKM19XNA6SJP36JZC065    Banana Republic   \n",
       "01DSJX8GD4DSAP76SPR85HRCMN              Loewe   \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2           Converse   \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE  Alexander McQueen   \n",
       "\n",
       "                                                            product_full_name  \\\n",
       "product_id                                                                      \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416                                   Ankle-Strap Pump   \n",
       "01DSE9SKM19XNA6SJP36JZC065                                Petite Tie-Neck Top   \n",
       "01DSJX8GD4DSAP76SPR85HRCMN               52MM Padded Leather Round Sunglasses   \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2  Baby's & Little Kid's All-Star Two-Tone Mid-To...   \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE                            64MM Rimless Sunglasses   \n",
       "\n",
       "                                                                  description  \\\n",
       "product_id                                                                      \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416  A modern pump, in a rounded silhouette with an...   \n",
       "01DSE9SKM19XNA6SJP36JZC065  Dress it down with jeans and sneakers or dress...   \n",
       "01DSJX8GD4DSAP76SPR85HRCMN    Padded leather covers classic round sunglasses.   \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2  The iconic mid-top design gets an added dose o...   \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE  Hexagonal shades offer a rimless view with int...   \n",
       "\n",
       "                                                               brand_category  \\\n",
       "product_id                                                                      \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416                                            Unknown   \n",
       "01DSE9SKM19XNA6SJP36JZC065                                            Unknown   \n",
       "01DSJX8GD4DSAP76SPR85HRCMN  JewelryAccessories/SunglassesReaders/RoundOval...   \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2  JustKids/Shoes/Baby024Months/BabyGirl,JustKids...   \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE     JewelryAccessories/SunglassesReaders/RoundOval   \n",
       "\n",
       "                                                          brand_canonical_url  \\\n",
       "product_id                                                                      \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416  https://bananarepublic.gap.com/browse/product....   \n",
       "01DSE9SKM19XNA6SJP36JZC065  https://bananarepublic.gap.com/browse/product....   \n",
       "01DSJX8GD4DSAP76SPR85HRCMN  https://www.saksfifthavenue.com/loewe-52mm-pad...   \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2  https://www.saksfifthavenue.com/converse-babys...   \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE  https://www.saksfifthavenue.com/alexander-mcqu...   \n",
       "\n",
       "                                                                      details  \\\n",
       "product_id                                                                      \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416  A modern pump, in a rounded silhouette with an...   \n",
       "01DSE9SKM19XNA6SJP36JZC065  Dress it down with jeans and sneakers or dress...   \n",
       "01DSJX8GD4DSAP76SPR85HRCMN  100% UV protection Case and cleaning cloth inc...   \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2  Canvas upper Round toe Lace-up vamp SmartFOAM ...   \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE  100% UV protection Gradient lenses Adjustable ...   \n",
       "\n",
       "                             category  \n",
       "product_id                             \n",
       "01DSE9TC2DQXDG6GWKW9NMJ416       Shoe  \n",
       "01DSE9SKM19XNA6SJP36JZC065   Onepiece  \n",
       "01DSJX8GD4DSAP76SPR85HRCMN  Accessory  \n",
       "01DSJVKJNS6F4KQ1QM6YYK9AW2       Shoe  \n",
       "01DSK15ZD4D5A0QXA8NSD25YXE  Accessory  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df5.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['style'] = df1.style_prediction\n",
    "df['fit'] = df2.fit\n",
    "df['occasion'] = df3.Occasion\n",
    "df['patterns/prints'] = full_data['patterns/prints']\n",
    "df['category'] = df5.category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
