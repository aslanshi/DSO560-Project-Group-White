{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This notebook contains the codes I wrote for DSO 560 Text Analytics & NLP Final Project to predict **style** for women clothing. The client is ThreadTogether, an Australian Non-profit orgnazation.\n",
    "\n",
    "Create on: 5.2.2020\n",
    "\n",
    "Create by: Nanchun (Aslan) Shi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.models import load_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dropout\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## functions:\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "punc = string.punctuation.replace('-','')\n",
    "\n",
    "def remove_punc_sw(text):\n",
    "    \n",
    "    for p in punc:\n",
    "        text = text.replace(p,' ')\n",
    "    text = text.replace('-', '')\n",
    "    text = text.replace(\"’\", ' ')\n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    filtered_tokens = list(filter(lambda token: token not in stopwords, tokens))\n",
    "    return \" \".join(filtered_tokens)\n",
    "\n",
    "def remove_numbers(text):\n",
    "    \n",
    "    text = re.sub(r'\\b\\d+\\b',' ',text)\n",
    "    text = re.sub(r'\\s+',' ',text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemma_pos_2(text):\n",
    "\n",
    "    for word, tag in pos_tag(text.split()):\n",
    "        if tag.startswith(\"N\"):\n",
    "            yield lemmatizer.lemmatize(word, wordnet.NOUN)\n",
    "        elif tag.startswith('V'):\n",
    "            yield lemmatizer.lemmatize(word, wordnet.VERB)\n",
    "        elif tag.startswith('J'):\n",
    "            yield lemmatizer.lemmatize(word, wordnet.ADJ)\n",
    "        elif tag.startswith('R'):\n",
    "            yield lemmatizer.lemmatize(word, wordnet.ADV)\n",
    "        else:\n",
    "            yield word\n",
    "            \n",
    "def combine_bigrams(doc):\n",
    "    \n",
    "    new_doc = []\n",
    "    \n",
    "    for w in doc:\n",
    "        w = w.replace('_','')\n",
    "        new_doc.append(w)\n",
    "        \n",
    "    return new_doc\n",
    "\n",
    "## function created by Professor Yu Chen\n",
    "\n",
    "def get_max_token_length_per_doc(docs):\n",
    "    return max(list(map(lambda x: len(x.split()), docs)))\n",
    "\n",
    "def get_pred_lists(model,df,thre = 0.7):\n",
    "    \n",
    "    prediction = model.predict(df)\n",
    "    preds = []\n",
    "    for p in prediction:\n",
    "        pred=[]\n",
    "        for i,v in enumerate(p):\n",
    "            if v >= thre:\n",
    "                pred.append(i)\n",
    "        if len(pred) == 0:\n",
    "            pred.append(p.argmax())\n",
    "        preds.append([pred])\n",
    "    return preds\n",
    "\n",
    "def check(x):\n",
    "    \n",
    "    pred = set(x[0])\n",
    "    true = set(x[1])\n",
    "    \n",
    "    if len(pred.intersection(true)) == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def accuracy(df):\n",
    "    \n",
    "    data = df.copy()\n",
    "    data['p_id_color_id'] = list(zip(data.index.get_level_values(0)\n",
    "                                           ,data.index.get_level_values(1)))\n",
    "    data['true'] = data.p_id_color_id.map(mapper2)\n",
    "    \n",
    "    return data[['pred','true']].apply(check,axis=1).sum()/len(data)\n",
    "\n",
    "punc2 = string.punctuation.replace('-','')\n",
    "\n",
    "def remove_punc_sw_combine(x):\n",
    "    \n",
    "    cols = [x[0],x[1],x[2],x[3]]\n",
    "    cleaned_cols = []\n",
    "    \n",
    "    for col in cols:\n",
    "        col = col.lower()\n",
    "        col = col.replace('\\n',' ')\n",
    "        for p in punc2:\n",
    "            col = col.replace(p,' ')\n",
    "        col = col.replace('-', '')\n",
    "        col = col.replace(\"’\", ' ')\n",
    "        cleaned_cols.append(col)\n",
    "        \n",
    "    return \" \".join(cleaned_cols)\n",
    "\n",
    "def lemma(text):\n",
    "    \n",
    "    new_list = []\n",
    "    for token in nltk.word_tokenize(text):\n",
    "        token = lemmatizer.lemmatize(token)\n",
    "        new_list.append(token)\n",
    "    return new_list\n",
    "\n",
    "def get_pred_classes(mat):\n",
    "    \n",
    "    pred = list(map(lambda v: list(np.argsort(v))[-2:], mat))\n",
    "#     pred = list(map(lambda v: [np.argsort(v)[-1]], mat))\n",
    "    return np.array(pred)\n",
    "\n",
    "def get_true_classes(df):\n",
    "    tclas=list()\n",
    "    for v in df.values:\n",
    "        tl = []\n",
    "        for i,a in enumerate(v):\n",
    "            if a == 1:\n",
    "                tl.append(i)\n",
    "        tclas.append(tl)\n",
    "    return tclas\n",
    "\n",
    "def compare(l1,l2):\n",
    "    \n",
    "    m=0\n",
    "    for i in range(len(l2)):\n",
    "        pred = set(l1[i])\n",
    "        true = set(l2[i])\n",
    "        \n",
    "        if len(pred.intersection(true)) != 0:\n",
    "            m += 1\n",
    "#         if (pred.issubset(true)) or (true.issubset(pred)):\n",
    "#             m+=1\n",
    "    \n",
    "    return m/len(l1) \n",
    "\n",
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['p_id','brand','mpn','p_full_name','description',\n",
    "           'brand_category','created_at','updated_at',\n",
    "           'deleted_at','brand_canonical_url','details',\n",
    "           'labels','bc_p_id','p_id_2','p_color_id','attribute_name',\n",
    "           'attribute_value','file']\n",
    "df = pd.read_csv('Tagged_data_2.csv',names=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## keep important fields\n",
    "\n",
    "df = df.iloc[:,np.r_[0:2,3:6,9:11,14:17]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select style tags\n",
    "\n",
    "sty_df = df[df.attribute_name=='style'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## There are lower cases\n",
    "\n",
    "dic1 = {key:key.lower().replace(' ','') for key in sty_df.attribute_value.unique()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert all attribute value to lower cases\n",
    "\n",
    "sty_df['attribute_value'] = sty_df['attribute_value'].map(dic1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set unique id as index\n",
    "\n",
    "sty_df.set_index(['p_id','p_color_id'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following sections are modeling. Given the different characteristics of **description, details**, and other text fields, I decided to treat them differently. Specifically, I will train an embedding neural network for description and detials, and train a normal MLP for other text fields. And lastly, I will take weighted average of predictions from both models and output final predictions by sorting the probabilities. \n",
    "\n",
    "NOTE: for style attribute, it is easily to find that a product could have multiple values. Therefore, when training the neural networks, my activation on the output layer would be \"sigmoid\" since it allows the \"independence\" from other neurons. The loss function is therefore \"binary_crossentropy\". The output values have shape of (X, 11), where X is the number of training set observations, and 11 is the number of unique style classes. For each observation, each number of the 11 numbers corresponds to the probability of being that class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Embedding\n",
    "\n",
    "Part I is the modeling for embedding neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select useful columns\n",
    "\n",
    "emb_df = sty_df.loc[:,['description','details','attribute_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## drop nulls ONLY if both of the fields are null\n",
    "\n",
    "emb_df.dropna(subset=['description','details'],how = 'all',inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## fill remaining nulls with NOINFO\n",
    "\n",
    "emb_df.fillna('NOINFO',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a unique key for mapping later\n",
    "\n",
    "emb_df['p_id_color_id'] = list(zip(emb_df.index.get_level_values(0)\n",
    "                                           ,emb_df.index.get_level_values(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a dummy dataframe (one-hot) for attribute values\n",
    "## sum if two rows have the same key; this means they are the same product but have multiple values\n",
    "## merge back to the orginal dataframe\n",
    "\n",
    "dummies = pd.get_dummies(emb_df.attribute_value)\n",
    "cum_dumm = dummies.groupby(dummies.index)[dummies.columns].sum().reset_index()\n",
    "emb_df = emb_df.merge(cum_dumm,left_on = 'p_id_color_id',right_on = 'index').drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## now in the orginal dataframe there will be duplicates; if one key has multiple values, there will be multiple\n",
    "## rows for that key, and values in each row are the same\n",
    "## so drop them\n",
    "\n",
    "emb_df.drop_duplicates(subset=['p_id_color_id'],inplace=True)\n",
    "emb_df.set_index(pd.Index(list(emb_df['p_id_color_id'])),inplace=True)\n",
    "emb_df.drop('p_id_color_id',axis=1,inplace=True)\n",
    "\n",
    "ys = emb_df.iloc[:,3:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## note: it could be the case that an entry is bigger than 1\n",
    "## this means in the original dataframe an unique key has multiple rows, but values were the same\n",
    "## so we want to change them to 1 since our loss function works for 0 and 1\n",
    "\n",
    "emb_df.iloc[:,3:] = np.where(ys > 1, 1, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## combine description and details\n",
    "\n",
    "emb_df['combined_text'] = emb_df[['description','details']].apply(lambda x: x[0]+' '+x[1],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Punctuation & Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df['cleaned_text'] = emb_df.combined_text.apply(remove_punc_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df['cleaned_text'] = emb_df.cleaned_text.apply(remove_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Lemmatization & POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lemmatize according to POS\n",
    "\n",
    "emb_df['lemma_text'] = emb_df['cleaned_text'].apply(lambda text: list(lemma_pos_2(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Find possible bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find possible bigrams and combine\n",
    "\n",
    "phrases = Phrases(emb_df.lemma_text, min_count=30)\n",
    "bi_gram = Phraser(phrases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_df['bigram_text'] = list(bi_gram[emb_df.lemma_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## bigrams are linked by \"_\", don't want it to be seperated later by the tokenizer\n",
    "## therefore remove it and connect the words with no space\n",
    "\n",
    "emb_df['bigram_text'] = emb_df['bigram_text'].apply(combine_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the phraser object for later use\n",
    "\n",
    "# save_obj(bi_gram, 'embedding_bigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.4 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a mapper where each unique key is associated with a list of true labels\n",
    "## will be used later\n",
    "\n",
    "check_df = emb_df.copy()\n",
    "mapper = check_df.groupby(check_df.index)['attribute_value'].apply(list)\n",
    "\n",
    "## create a label dictionary where keys are labels, and values are number 0-10\n",
    "\n",
    "label_dic = dict(zip(emb_df.iloc[:,3:-4].columns, range(11)))\n",
    "labels = emb_df.attribute_value.map(label_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save label dictionary\n",
    "\n",
    "# save_obj(label_dic,'style_label_dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create another mapper where change the list of true lables in the first mapper to corresponding numbers\n",
    "\n",
    "num_labels = []\n",
    "for l in mapper:\n",
    "    num = []\n",
    "    for v in l:\n",
    "        num.append(label_dic[v])\n",
    "    num_labels.append(num)\n",
    "    \n",
    "mapper2 = pd.Series(num_labels,index=mapper.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train own embeddings using NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## join the tokens\n",
    "\n",
    "emb_df['joined_text'] = emb_df.bigram_text.apply(lambda l: \" \".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "## maximum text length\n",
    "\n",
    "max_length = get_max_token_length_per_doc(emb_df.joined_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT USED\n",
    "\n",
    "# all_words = []\n",
    "\n",
    "# for l in emb_df.bigram_text:\n",
    "#     for token in l:\n",
    "#         all_words.append(token)\n",
    "\n",
    "# unique_words = int(len(set(all_words))*1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## instantize tokenizer from keras; fit on our corpus\n",
    "\n",
    "tk = Tokenizer(oov_token = 'UNKNOWN_TOKEN')\n",
    "tk.fit_on_texts(emb_df.joined_text)\n",
    "vocab_size = len(tk.word_index) + 1\n",
    "\n",
    "## transform and padding\n",
    "\n",
    "vector_text = tk.texts_to_sequences(emb_df.joined_text)\n",
    "padded_token_lists = pad_sequences(vector_text, maxlen=max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOT USED\n",
    "## integer encodin & padding\n",
    "\n",
    "# vector_text = [one_hot(v, unique_words) for v in emb_df.joined_text]\n",
    "# padded_token_lists = pad_sequences(vector_text, max_length, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## crate dataframe so that after sklearn train test split, indices could be kept\n",
    "\n",
    "df_own_x = pd.DataFrame(padded_token_lists, index = emb_df.index)\n",
    "df_own_y = pd.DataFrame(emb_df.iloc[:,3:-5], index = emb_df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## set random state because we want to use the same docs for training tf-idf model\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_own_x,df_own_y, \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build sequential model with embedding layer\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length))\n",
    "# model.add(Embedding(input_dim=vocab_size, output_dim=100, input_length=max_length, mask_zero = True))\n",
    "model.add(Flatten())\n",
    "# model.add(LSTM(100))\n",
    "model.add(Dense(100,activation='tanh'))\n",
    "model.add(Dense(11, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aslanshi/opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15b0e6390>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## compile and fit\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model.fit(X_train,y_train,batch_size=200,epochs=100,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the predicted probabilities for train and test set for use later\n",
    "\n",
    "emb_pred_vectors_train = model.predict(X_train)\n",
    "emb_pred_vectors_test = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## retrieve embedding layer weights\n",
    "\n",
    "embeddings = model.layers[0].get_weights()[0]\n",
    "embedding_dict = {w:embeddings[idx] for w, idx in tk.word_index.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save embedding weights and tokenizer object\n",
    "\n",
    "# save_obj(embedding_dict,'emb_dict')\n",
    "# save_obj(tk,'tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "\n",
    "# model.save('embedding_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "## select columns\n",
    "\n",
    "tfidf_df = sty_df.loc[:,['brand','p_full_name','brand_category','brand_canonical_url','description','details','attribute_value']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## because we drop row that miss both description and detials, we need to do the same here in order to \n",
    "## be consistent\n",
    "\n",
    "tfidf_df.dropna(subset=['description','details'],how = 'all',inplace = True)\n",
    "tfidf_df.drop(['description','details'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.fillna('NOINFO',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "## the following few cells are doing the same as before, one could refer to the previous section\n",
    "\n",
    "tfidf_df['p_id_color_id'] = list(zip(tfidf_df.index.get_level_values(0)\n",
    "                                           ,tfidf_df.index.get_level_values(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies = pd.get_dummies(tfidf_df.attribute_value)\n",
    "cum_dumm = dummies.groupby(dummies.index)[dummies.columns].sum().reset_index()\n",
    "tfidf_df = tfidf_df.merge(cum_dumm,left_on = 'p_id_color_id',right_on = 'index').drop('index',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.drop_duplicates(subset=['p_id_color_id'],inplace=True)\n",
    "tfidf_df.set_index(pd.Index(list(tfidf_df['p_id_color_id'])),inplace=True)\n",
    "tfidf_df.drop('p_id_color_id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = tfidf_df.iloc[:,5:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.iloc[:,5:] = np.where(ys > 1, 1, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df['all_text'] = tfidf_df[['brand','p_full_name','brand_category','brand_canonical_url']].apply(remove_punc_sw_combine,axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Remove numbers & website characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df.all_text = tfidf_df.all_text.str.replace(r'(\\b\\d+\\b)','')\n",
    "tfidf_df.all_text = tfidf_df.all_text.str.replace(r'(\\swww\\s|\\shttps*\\s)','')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df['lemma_text'] = tfidf_df.all_text.apply(lemma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 Bi-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrases3 = Phrases(tfidf_df.lemma_text, min_count=10)\n",
    "bi_gram3 = Phraser(phrases3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df['bigram_text']=list(bi_gram3[tfidf_df.lemma_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df['bigram_text'] = tfidf_df['bigram_text'].apply(combine_bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_obj(bi_gram3, 'tfidf_bigram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.5 Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_df['joined_text'] = tfidf_df.bigram_text.apply(lambda l: \" \".join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## instantize tfidfvectorizer from sklearn, set max_features to 500 to reduce dimensionality \n",
    "## and keep important tokens\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=500)\n",
    "\n",
    "## fit and transform\n",
    "\n",
    "X = vectorizer.fit_transform(tfidf_df.joined_text)\n",
    "terms = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = pd.DataFrame(X.toarray(), columns = terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.6 Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAME random state as embedding\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tf_idf.values, tfidf_df.iloc[:,5:-4], \n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## build a sequential model\n",
    "\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(100,activation='relu',input_shape = (500,)))\n",
    "model2.add(Dense(11, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x15d0aa050>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## complie and fit\n",
    "\n",
    "model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "model2.fit(X_train,y_train,batch_size=100,epochs=100,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save the predicted probabilities for use later\n",
    "\n",
    "tfidf_pred_vectors_train = model2.predict(X_train)\n",
    "tfidf_pred_vectors_test = model2.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "\n",
    "# model2.save('tfidf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save vectorizer object\n",
    "\n",
    "# save_obj(vectorizer, 'tfidf_vectorizer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Combine them together\n",
    "#### Average & Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "## take weighted average of two predictied vectors for train and test\n",
    "\n",
    "final_vectors_train = 0.4*emb_pred_vectors_train + 0.6*tfidf_pred_vectors_train\n",
    "final_vectors_test = 0.4*emb_pred_vectors_test + 0.6*tfidf_pred_vectors_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "## convert the final predicted vectors to corresponding classes\n",
    "## see function at the beginning for detials\n",
    "\n",
    "train_pred_classes = get_pred_classes(final_vectors_train)\n",
    "test_pred_classes = get_pred_classes(final_vectors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "## get true classess for each observation\n",
    "## see function at the beginning for details\n",
    "\n",
    "train_true_classes = get_true_classes(y_train)\n",
    "test_true_classes = get_true_classes(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9997362869198312"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## note the function check if there is intersection of predicted classes and true classes\n",
    "## so there would be false positives; after checking, not too much\n",
    "## if only output on class, training accuracy would be around 96% and test accuracy would be around 90%\n",
    "\n",
    "## if we output two classes for each product\n",
    "## training accuracy\n",
    "\n",
    "compare(train_pred_classes,train_true_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9618696186961869"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## if output 2 classes\n",
    "## test accuracy\n",
    "\n",
    "compare(test_pred_classes,test_true_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
